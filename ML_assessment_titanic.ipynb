{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a85472f3",
   "metadata": {},
   "source": [
    "# MICROCREDENTIAL PROGRAM MACHINE LEARNING ASSESSMENT\n",
    "\n",
    "The Titanic embarked from England in April 1912 and was destined for New York City. However it did not make it. In this assessment you will implement a machine learning analysis on data about the passengers on board to predict their survival.\n",
    "\n",
    "The data is from Kaggle:\n",
    "https://www.kaggle.com/c/titanic\n",
    "\n",
    "Survival is a **binary outcome**, meaning there are two possible discrete outcomes (exactly zero or exactly 1, unlike for example a house price which can be any positive value). There are many different approaches to take to predict binary outcomes.\n",
    "\n",
    "Most of the analysis is filled out but throughout the notebook there are questions for you to answer and some code for you to fill in. To answer the written questions just click on the markdown cells and type your answers there.\n",
    "\n",
    "Towards the end you will decide which machine learning algorithm to implement in order to predict survival of the Titanic passengers.\n",
    "\n",
    "This reflects the house regression code from the ML3 session so looking through that notebook can be helpful to assist you on this assessment:\\\n",
    "https://github.com/akshayghosh-acenet/IntroMachineLearning3\n",
    "\n",
    "And of course if you have any questions whether they are conceptual or about coding feel free to send me a message (:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ed605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# you can import more libraries here or thoughout the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "fn_train = 'train_titanic.csv'\n",
    "fn_test = 'test_titanic.csv'\n",
    "\n",
    "train_data = pd.read_csv(fn_train) # this has known outcomes, used for training and tuning\n",
    "test_data = pd.read_csv(fn_test) # this does not have known outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1430e0c8",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "With the data loaded, we now want to do the very important first step of any ML analysis: exploratory data analysis or EDA.\n",
    "\n",
    "There are many different things that can be done in EDA, here we will look at the basic statistics (i.e. mean, median, etc) along with histograms of the numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9294a0c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# a good first step is to do dataframe.head() to see generally what the data looks like\n",
    "\n",
    "train_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a896677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use dataframe.describe() to get statistics on each of the columns\n",
    "\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54a9e58",
   "metadata": {},
   "source": [
    "### Variables explained:\n",
    "\n",
    "**Below are explanations of the variables/parameters in the dataset. Think about which ones you think should be included in the analysis, you will need to decide this later.**\n",
    "\n",
    "| Variable | Definition | Key |\n",
    "|---|---|---|\n",
    "| Survived | Did they survive | 0 = no, 1 = yes |\n",
    "| Pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "| Sex | sex | - |\n",
    "| Age | Age in years | - |\n",
    "| SibSp| # of siblings/spouses on board | - |\n",
    "| Parch | # of parents/children on board | - |\n",
    "| Ticket | Ticket number | - |\n",
    "| Fare | Passenger fare | - |\n",
    "| Cabin | Cabin Nuber | - |\n",
    "| Embarked | Port left from | C = Cherbourg, Q = Queenstown, S = Southampton |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036d65c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# EDA\n",
    "\n",
    "# here we use a for loop to plot some histograms\n",
    "\n",
    "for parameter in ['Pclass','Sex','Age','Fare','SibSp','Parch']:\n",
    "    plt.figure()\n",
    "    plt.hist(train_data[parameter],bins = 20)\n",
    "    plt.xlabel(parameter)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bd8dec",
   "metadata": {},
   "source": [
    "## QUESTION 1: _How many NaNs?_\n",
    "\n",
    "An important part of analysis is determining what percentage of your dataset are NaN values (not a number, i.e. a missing value). In the cell below do the following:\n",
    "\n",
    "**i. Print the name of each column along with its number of NaN values.**\n",
    "\n",
    "**ii. Out of the total number of datapoints ($n_{rows} \\times n_{cols}$), what percentage of these are NaNs? Calculate the total number of NaNs and divide by the total number of datapoints and multiply this result by 100 to get it in a percentage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b31a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess NaNs\n",
    "\n",
    "# print each column name along with its number of NaN values here:\n",
    "\n",
    "\n",
    "\n",
    "# calculate percentage of NaNs in the dataset here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b7367e",
   "metadata": {},
   "source": [
    "## QUESTION 2: _Feature selection_\n",
    "\n",
    "Here you are to select the features (i.e. columns or variables) that you would like to include in the analysis. You will also split the dataset into training and validation data (in this case the \"testing\" dataset is the data with unknown outcomes, just like in the housing regression from the ML3 session).\n",
    "\n",
    "**i. Select the features by creating a list of strings of the feature names that you would like to include. In the cell below the code write a sentence about why you decided to include or not include each featuure (there are some that do not need to be included but this is up to your discretion!).**\n",
    "\n",
    "**ii. Split the feature array and target array ($X$ and $y$ respectively) into training and validation subsets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f6127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split for training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = # PUT LIST OF FEATURES HERE\n",
    "\n",
    "X = train_data[features] # set the features to be the columns in the list\n",
    "y = train_data['Survived'] # set the target to be Survived\n",
    "\n",
    "# split the training data into training and validation subsets\n",
    "X_train, X_val, y_train, y_val = # SPLIT DATA HERE\n",
    "\n",
    "# this is just calling the testing features X_test for symmetry with the training data. \n",
    "# there is no y_test, that is for us to calculate\n",
    "X_test = test_data[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b1bfc1",
   "metadata": {},
   "source": [
    "### Question 2 continued:\n",
    "\n",
    "Feature justification:\n",
    "\n",
    "**iii. After the dash write a sentence or two on why you decided to include/exclude that variable for predicting the survival outcome. Think about, does this information have an effect on surviving the Titanic sinking?**\n",
    "\n",
    "PassengerId -\n",
    "\n",
    "Pclass -\n",
    "\n",
    "Name -\n",
    "\n",
    "Sex -\n",
    "\n",
    "Age -\n",
    "\n",
    "SibSp -\n",
    "\n",
    "Parch -\n",
    "\n",
    "Ticket -\n",
    "\n",
    "Fare -\n",
    "\n",
    "Cabin -\n",
    "\n",
    "Embarked -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423633f6",
   "metadata": {},
   "source": [
    "## QUESTION 3: _Data preprocessing_\n",
    "\n",
    "Here the data preprocessing is set up just like how we did in the ML3 session.\n",
    "\n",
    "It is mostly filled out, but here is what you need to do:\n",
    "\n",
    "**i: Research for and select a method to impute the numerical data. Here is a starting point**\n",
    "https://scikit-learn.org/stable/modules/impute.html\n",
    "\n",
    "**ii: Research for and select a method to scale the numerical data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f879646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING STEPS FOR NUMERICAL AND CATEGORICAL FEATURES\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import # import imputing functions here\n",
    "from sklearn.preprocessing import # import scaling and encoding functions here\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# this gets the numeric features as a list, these will either be an integer or a float\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# similarly this gets the categorical features as a list. these are strings and not numbers, for example could be 'yes' or 'no'\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# this defines how we want to transform the numerical features\n",
    "\n",
    "# !!!PUT YOUR IMPUTER AND SCALERS FOR THE NUMERICAL DATA HERE!!!:\n",
    "NUMERICAL_IMPUTER = # find a sklearn imputer and put it here like NUMERICAL_IMPUTER = ImputerMethod(Parameters)\n",
    "NUMERICAL_SCALER = #  find a sklearn scalar and put it here like NUMERICAL_SCALER = ScalerMethod(Parameters)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', NUMERICAL_IMPUTER), # here we replace every missing or NaN value with the mean of the features\n",
    "    ('scaler', NUMERICAL_SCALER) # here we scale every feature such that its mean is 0 and standard deviation is 1\n",
    "])\n",
    "\n",
    "\n",
    "# this defines how we want to transform the categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # here we replace every missing or NaN value with the most commonly occuring label\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # this converts the categorical variables\n",
    "])\n",
    "\n",
    "\n",
    "# this now applies the column transforms we just defined\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features), # define the process as the name 'num', and tell sklearn to apply the numeric transformer we defined the the numeric features we got into the list\n",
    "        ('cat', categorical_transformer, categorical_features) # same with the categorical variables\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee788630",
   "metadata": {},
   "source": [
    "### Question 3 continued\n",
    "\n",
    "Answer the following questions in a couple of sentences:\n",
    "\n",
    "**iii. What is imputation and why is it useful/important in data preprocessing?**\n",
    "\n",
    "**iv. Explain what one-hot encoding and when you would implement it in data preprocessing.**\n",
    "\n",
    "**v. Explain why scaling the numerical data is important in data preprocessing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb3447",
   "metadata": {},
   "source": [
    "## QUESTION 4: _Model selection_\n",
    "\n",
    "Now for the fun part! Here you are to determine which sklearn ML model to implement for the analysis.\n",
    "\n",
    "Some example libraries to find useful models are `sklearn.ensemble` or `sklearn.linear_model`.\n",
    "\n",
    "This link could be a useful start for determining a model: https://scikit-learn.org/stable/tutorial/machine_learning_map/\n",
    "\n",
    "Note that you do  not have to be limited to `sklearn` for your model! There are other Python ML libraries if you want to venture out and choose another model/algorithm.\n",
    "\n",
    "**i. Choose and implement a machine learning model to predict survival (remember it is a _binary outcome_) for the passengers on board the Titanic.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d355dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "\n",
    "# IMPORT MODEL HERE\n",
    "\n",
    "model = # DEFINE MODEL HERE\n",
    "model_name = '' # put a name for your model to input to the Pipeline function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f6f2b7",
   "metadata": {},
   "source": [
    "### Question 4 continued:\n",
    "\n",
    "**ii. Write a short paragraph justifying why you chose the model that you did.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b98931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define full model\n",
    "\n",
    "full_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              (model_name, model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb26a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model to data\n",
    "\n",
    "full_model.fit(X_train, y_train)\n",
    "None # I just put this here to suppress the output of full_model.fit(X_train, y_train). if you want you can delete this line and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d54d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "\n",
    "y_val_pred = full_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6e482",
   "metadata": {},
   "source": [
    "In the cell below the accuracy metrics are calculated by comparing what is called the \"ground truth\" (the known values for survival in `y_val`, to the predicted values that were obtained by applying the model to `X_val` (we call this `y_val_pred`).\n",
    "\n",
    "Feel free to go back and select a different model, or adjust the parameters, and see how the accuracy metrics and confusion matrix change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87592ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "y_true = y_val\n",
    "y_pred = y_val_pred\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "# Calculate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred) \n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Confusion matrix:\")\n",
    "\n",
    "# this plots the confusion matrix\n",
    "cmd = ConfusionMatrixDisplay(conf_matrix)\n",
    "fig, ax = plt.subplots(dpi = 300)\n",
    "cmd.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f159bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use the model to make predictions on the test data!\n",
    "\n",
    "y_unknown_predict = full_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6705d4f",
   "metadata": {},
   "source": [
    "### QUESTION 5:\n",
    "\n",
    "**i. Write a sentence or two explaining each of the following:**\n",
    "\n",
    "Accuracy - \n",
    "\n",
    "Precision - \n",
    "\n",
    "Recall - \n",
    "\n",
    "Confusion matrix -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee568d1",
   "metadata": {},
   "source": [
    "## BONUS QUESTION 6 (OPTIONAL): _Hyperparameter tuning_\n",
    "\n",
    "**i. In the cells below tune the parameters in the model you selected and recalculate the accuracy metrics. Make a comment on the method for hyperparameter tuning (i.e. grid search, random search, etc) that you chose.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31387788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning goes here and below"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
